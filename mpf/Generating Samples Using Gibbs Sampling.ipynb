{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import time\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall consider a network with $v$ vertices where each vertex is binary. For a network with $v$ vertices, there are $2^v$ possibles binary states. We initialize an initial state of the network using the Bernoulli distribution with success probability $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of neurons:\n",
    "v = 16\n",
    "# Success probability:\n",
    "p = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needs to be edited later!!!\n",
    "We now initialize a $v$ by $v$ matrix $W$ with each entry drawn from a standard normal distribution, $N(0,1)$. For each entry in the matrix, $W_{ij}$ denotes the parameter/weight associated with the connection from unit $i$ to $j$ (can we think of it as the conditional weight of $v^{(t+1)}_i=1$ given $v^{(t)}_j=1$?). Here we save the matrix $W$ so we can verify the learning by MPF. \n",
    "\n",
    "(Personal notes: Later, we will learn that initializing the matrix $W$ with zero diagonals will make it easier in the generation of samples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initializestates(v, p):\n",
    "    \"\"\"\n",
    "    Initializes a v by 1 array of initial states\n",
    "    Input:\n",
    "    - v: (int) number of neurons in the system.\n",
    "    - p: (probability) probability of getting a 1.\n",
    "    \"\"\"\n",
    "    initialState = np.random.binomial(1, p, v)\n",
    "    initialState = initialState.reshape(1, v)\n",
    "    return initialState\n",
    "\n",
    "\n",
    "def initializeW(v):\n",
    "    \"\"\"\n",
    "    Initializes a v by v matrix of values. For now we can think of it\n",
    "    in the scenario of a Ising model where W describes the interaction\n",
    "    between the different nodes.\n",
    "    Input:\n",
    "    - v: (int) number of neurons in the system.\n",
    "    \"\"\"\n",
    "    W = np.random.normal(0, 1, (v, v))\n",
    "    W = np.triu(W, 1)+np.triu(W, 1).T\n",
    "    # To save and load W matrix\n",
    "    np.save('W.dat', W)\n",
    "    # W = np.load('W.dat')\n",
    "    print ('Initialized W matrix: \\n', W)\n",
    "    return W\n",
    "\n",
    "\n",
    "def initializeb(v):\n",
    "    \"\"\"\n",
    "    Initializes a 1 by v array of values. For now we can think of it\n",
    "    some form of bias in the system.\n",
    "    Input:\n",
    "    - v: (int) number of neurons in the system.\n",
    "    \"\"\"\n",
    "    b = np.zeros((1,v))\n",
    "    # W = np.random.normal(0, 1, (v,v))\n",
    "    # W = 0.5 * (W + np.transpose(W))\n",
    "    # W = W - np.diag(W)\n",
    "\n",
    "    # To save and load W matrix\n",
    "    # np.save('W.dat', W)\n",
    "    # W = np.load('W.dat')\n",
    "    print ('Initialized bias: ', b)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do Gibbs Sampling\n",
    "T\n",
    "histributions $v$ times for a new state of the network to be obtained., preason for doing Gibbs sampling is to generate samples $\\mathcal{S}$ from known parameters $W$ and then use MPF to learn the parameters $W$ using $\\mathcal{S}$. To sample from this multivariate distribution, we start with an initial state obtained from a prior belief following which sampling from the conditional distribution is done to get a new state of a **vertex**. \n",
    "\n",
    "#### Algorithm: Gibbs sampler (random scan)\n",
    "1. Initialize $\\mathbf{x^{(0)}}=(x_1^{(0)},\\ldots,x_v^{(0)})$ base on some prior belief.\n",
    "2. For $i = 1,2, \\ldots$, pick a random integer $k$ from $1 , \\ldots, v$ then \n",
    "    - sample $X_k^{(i)}\\sim \\mathbb{P}(X_k^{(i)}=x_1^{(i)}\\mid X_1=x_1^{(i-1)},X_2=x_2^{(i-1)},\\ldots,X_{k-1}=x_{k-1}^{(i-1)},X_{k+1}=x_{k+1}^{(i-1)},\\ldots,X_v=x_v^{(i-1)})$\n",
    "which gives you a new state of the network.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Takes in a vector x and returns its sigmoid activation.\n",
    "    Input:\n",
    "    - x: a numpy array\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def single_unit_update(initialState, W, b, v):\n",
    "    \"\"\"\n",
    "    Returns the new states and the state of the vth vertex that has been updated conditioned on the other units\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array of values that the prior distribution is based from.\n",
    "    - b: a (1, v) numpy array of bias\n",
    "    - v: (int) the state of the vertex to be updated.\n",
    "    \"\"\"\n",
    "    stateSize = initialState.shape\n",
    "    newState = np.zeros(stateSize) + initialState\n",
    "    prob = sigmoid(initialState.dot(W) + b)\n",
    "    newState[0, v] = np.random.binomial(1, prob[0, v], 1)\n",
    "    return newState, newState[0, v]\n",
    "\n",
    "\n",
    "def rand_gibbs_sample(initialState, W, b, n):\n",
    "    \"\"\"\n",
    "    Does a random scan Gibbs sampling n times with a given initial state, weight matrix W and bias b.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array.\n",
    "    - b: a (1, v) numpy array of bias\n",
    "    - n: (int) number of samples to be generated.\n",
    "    \"\"\"\n",
    "    for i in range(n):\n",
    "        s = np.random.randint(0, v)\n",
    "        initialState, vertexState = single_unit_update(initialState, W, b, s)\n",
    "    return initialState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make ensure that the sample that we obtain are independent and identically distributed, we do a **burn-in** of $10000\\times v$ iterations so that the samples obtained follow the distribution of the weight matrix, following which we pick a sample for every $1000 \\times v$ iterations, which is called **mixing-in**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def burnin(initialState, W, b):\n",
    "    \"\"\"\n",
    "    Performs burn in of 10000 x v iterations.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array.\n",
    "    - b: a (1, v) numpy array of bias\n",
    "    \"\"\"\n",
    "    v = W.shape[0]\n",
    "    burnin_state = rand_gibbs_sample(initialState, W, b, 10000 * v)\n",
    "    print ('Burn-in state: ', burnin_state)\n",
    "    return burnin_state\n",
    "\n",
    "\n",
    "def mixin_gibbs_sample(initialState, W, b, n, m, savesamples = 'True'):\n",
    "    \"\"\"\n",
    "    Does a random scan Gibbs sampling n * m times with a given initial state and weight matrix W and \n",
    "    stores a sample every m iterations.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array. \n",
    "    - n: (int) number of samples to be drawn.\n",
    "    - m: (int) number of iterations before a sample is drawn.\n",
    "    - savedate: (bool) save samples as 'samples.dat.npy' if True and does not save if false.\n",
    "    \"\"\"\n",
    "    tic = time.time()\n",
    "           \n",
    "    v = W.shape[0]\n",
    "    sample = np.zeros((n, initialState.shape[1]))\n",
    "    for i in range(n):\n",
    "        initialState = rand_gibbs_sample(initialState, W, b, m)\n",
    "        sample[i] = initialState\n",
    "    if savesamples == \"True\":\n",
    "        np.save('gibbs-sample.dat', sample)\n",
    "        print ('Samples are saved as \"gibbs-sample.dat.npy\"')\n",
    "    elif savesamples == \"False\":\n",
    "        print ('Samples were not saved. Run np.save(\"gibbs-sample.dat\", sample) to save them. ')\n",
    "    else:\n",
    "        raise ValueError(\"savesamples must be 'True' or 'False'\")\n",
    "    \n",
    "    toc = time.time()\n",
    "    print ('Time taken to create %d samples is %f minutes' % (n, (toc - tic)/60))\n",
    "    return sample\n",
    "\n",
    "def makesamples(initialState, W, b, n, m, savesamples = 'True'):\n",
    "    \"\"\"\n",
    "    Make samples.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array. \n",
    "    - n: (int) number of samples to be drawn.\n",
    "    - m: (int) number of iterations before a sample is drawn.\n",
    "    - savedate: (bool) save samples as 'samples.dat.npy' if True and does not save if false.\n",
    "    \"\"\"\n",
    "    b = burnin(initialState, W, b)\n",
    "    samples = mixin_gibbs_sample(b, W, b, n, m, savesamples)\n",
    "    return samples  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized W matrix: \n",
      " [[ 0.          0.56276399  0.58425855  0.27599696  0.22047828  0.77678382\n",
      "   0.31466923  0.75002074  0.47031397  0.91911901  0.37530959  0.21830439\n",
      "   0.14898996  0.48076842  0.67725412  0.29028265]\n",
      " [ 0.56276399  0.          0.41409302  0.54647256  0.52263074  0.6018849\n",
      "   0.68095789  0.27050287  0.46015778  0.6759992   0.54026853  0.69580176\n",
      "   0.08856579  0.56237861  0.22555821  0.62433155]\n",
      " [ 0.58425855  0.41409302  0.          0.26388497  0.9040701   0.67112882\n",
      "   0.48501629  0.61482975  0.78331851  0.57773768  0.54590006  0.5308558\n",
      "   0.77097628  0.84167465  0.54069942  0.22616524]\n",
      " [ 0.27599696  0.54647256  0.26388497  0.          0.72825358  0.64482585\n",
      "   0.38486102  0.51483597  0.78210032  0.62095867  0.68180481  0.27180626\n",
      "   0.67420465  0.25886465  0.6935016   0.50376572]\n",
      " [ 0.22047828  0.52263074  0.9040701   0.72825358  0.          0.65961549\n",
      "   0.7874995   0.49129188  0.19573179  0.11278413  0.41042244  0.5064362\n",
      "   0.43105644  0.28227311  0.55799279  0.51273343]\n",
      " [ 0.77678382  0.6018849   0.67112882  0.64482585  0.65961549  0.\n",
      "   0.31291781  0.62412609  0.62207147  0.36802817  0.18420644  0.3862262\n",
      "   0.55868693  0.75959839  0.42322509  0.38313647]\n",
      " [ 0.31466923  0.68095789  0.48501629  0.38486102  0.7874995   0.31291781\n",
      "   0.          0.35773824  0.53886732  0.83741275  0.56811429  0.49995319\n",
      "   0.37735246  0.48339235  0.6151154   0.29459209]\n",
      " [ 0.75002074  0.27050287  0.61482975  0.51483597  0.49129188  0.62412609\n",
      "   0.35773824  0.          0.2316723   0.20953839  0.18082482  0.25980544\n",
      "   0.27384924  0.22546391  0.62193675  0.41478734]\n",
      " [ 0.47031397  0.46015778  0.78331851  0.78210032  0.19573179  0.62207147\n",
      "   0.53886732  0.2316723   0.          0.01061587  0.44038171  0.54935247\n",
      "   0.84400109  0.56276463  0.82734151  0.52418768]\n",
      " [ 0.91911901  0.6759992   0.57773768  0.62095867  0.11278413  0.36802817\n",
      "   0.83741275  0.20953839  0.01061587  0.          0.93005726  0.41445156\n",
      "   0.57768929  0.62613294  0.46175452  0.65972251]\n",
      " [ 0.37530959  0.54026853  0.54590006  0.68180481  0.41042244  0.18420644\n",
      "   0.56811429  0.18082482  0.44038171  0.93005726  0.          0.35031522\n",
      "   0.87599189  0.57976824  0.53867475  0.29321206]\n",
      " [ 0.21830439  0.69580176  0.5308558   0.27180626  0.5064362   0.3862262\n",
      "   0.49995319  0.25980544  0.54935247  0.41445156  0.35031522  0.\n",
      "   0.52192929  0.20507305  0.22153888  0.85158802]\n",
      " [ 0.14898996  0.08856579  0.77097628  0.67420465  0.43105644  0.55868693\n",
      "   0.37735246  0.27384924  0.84400109  0.57768929  0.87599189  0.52192929\n",
      "   0.          0.59981398  0.52827258  0.4040388 ]\n",
      " [ 0.48076842  0.56237861  0.84167465  0.25886465  0.28227311  0.75959839\n",
      "   0.48339235  0.22546391  0.56276463  0.62613294  0.57976824  0.20507305\n",
      "   0.59981398  0.          0.25882051  0.76823455]\n",
      " [ 0.67725412  0.22555821  0.54069942  0.6935016   0.55799279  0.42322509\n",
      "   0.6151154   0.62193675  0.82734151  0.46175452  0.53867475  0.22153888\n",
      "   0.52827258  0.25882051  0.          0.19623454]\n",
      " [ 0.29028265  0.62433155  0.22616524  0.50376572  0.51273343  0.38313647\n",
      "   0.29459209  0.41478734  0.52418768  0.65972251  0.29321206  0.85158802\n",
      "   0.4040388   0.76823455  0.19623454  0.        ]]\n",
      "Initialized bias:  [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "Burn-in state:  [[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]]\n",
      "Samples are saved as \"gibbs-sample.dat.npy\"\n",
      "Time taken to create 50000 samples is 2.423147 minutes\n"
     ]
    }
   ],
   "source": [
    "run mpfgibbs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
