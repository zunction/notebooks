{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import time\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall consider a network with $v$ vertices where each vertex is binary. For a network with $v$ vertices, there are $2^v$ possibles binary states. We initialize an initial state of the network using the Bernoulli distribution with success probability $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of neurons:\n",
    "v = 16\n",
    "# Success probability:\n",
    "p = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needs to be edited later!!!\n",
    "We now initialize a $v$ by $v$ matrix $W$ with each entry drawn from a standard normal distribution, $N(0,1)$. For each entry in the matrix, $W_{ij}$ denotes the parameter/weight associated with the connection from unit $i$ to $j$ (can we think of it as the conditional weight of $v^{(t+1)}_i=1$ given $v^{(t)}_j=1$?). Here we save the matrix $W$ so we can verify the learning by MPF. \n",
    "\n",
    "(Personal notes: Later, we will learn that initializing the matrix $W$ with zero diagonals will make it easier in the generation of samples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initializestates(v, p):\n",
    "    \"\"\"\n",
    "    Initializes a v by 1 array of initial states\n",
    "    Input:\n",
    "    - v: (int) number of neurons in the system.\n",
    "    - p: (probability) probability of getting a 1.\n",
    "    \"\"\"\n",
    "    initialState = np.random.binomial(1, p, v)\n",
    "    initialState = initialState.reshape(1, v)\n",
    "    return initialState\n",
    "\n",
    "\n",
    "def initializeW(v):\n",
    "    \"\"\"\n",
    "    Initializes a v by v matrix of values. For now we can think of it\n",
    "    in the scenario of a Ising model where W describes the interaction\n",
    "    between the different nodes.\n",
    "    Input:\n",
    "    - v: (int) number of neurons in the system.\n",
    "    \"\"\"\n",
    "    W = np.random.normal(0, 1, (v, v))\n",
    "    W = np.triu(W, 1)+np.triu(W, 1).T\n",
    "    # To save and load W matrix\n",
    "    np.save('W.dat', W)\n",
    "    # W = np.load('W.dat')\n",
    "    print ('Initialized W matrix: \\n', W)\n",
    "    return W\n",
    "\n",
    "\n",
    "def initializeb(v):\n",
    "    \"\"\"\n",
    "    Initializes a 1 by v array of values. For now we can think of it\n",
    "    some form of bias in the system.\n",
    "    Input:\n",
    "    - v: (int) number of neurons in the system.\n",
    "    \"\"\"\n",
    "    b = np.zeros((1,v))\n",
    "    # W = np.random.normal(0, 1, (v,v))\n",
    "    # W = 0.5 * (W + np.transpose(W))\n",
    "    # W = W - np.diag(W)\n",
    "\n",
    "    # To save and load W matrix\n",
    "    # np.save('W.dat', W)\n",
    "    # W = np.load('W.dat')\n",
    "    print ('Initialized bias: ', b)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do Gibbs Sampling\n",
    "T\n",
    "histributions $v$ times for a new state of the network to be obtained., preason for doing Gibbs sampling is to generate samples $\\mathcal{S}$ from known parameters $W$ and then use MPF to learn the parameters $W$ using $\\mathcal{S}$. To sample from this multivariate distribution, we start with an initial state obtained from a prior belief following which sampling from the conditional distribution is done to get a new state of a **vertex**. \n",
    "\n",
    "#### Algorithm: Gibbs sampler (random scan)\n",
    "1. Initialize $\\mathbf{x^{(0)}}=(x_1^{(0)},\\ldots,x_v^{(0)})$ base on some prior belief.\n",
    "2. For $i = 1,2, \\ldots$, pick a random integer $k$ from $1 , \\ldots, v$ then \n",
    "    - sample $X_k^{(i)}\\sim \\mathbb{P}(X_k^{(i)}=x_1^{(i)}\\mid X_1=x_1^{(i-1)},X_2=x_2^{(i-1)},\\ldots,X_{k-1}=x_{k-1}^{(i-1)},X_{k+1}=x_{k+1}^{(i-1)},\\ldots,X_v=x_v^{(i-1)})$\n",
    "which gives you a new state of the network.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Takes in a vector x and returns its sigmoid activation.\n",
    "    Input:\n",
    "    - x: a numpy array\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def single_unit_update(initialState, W, b, v):\n",
    "    \"\"\"\n",
    "    Returns the new states and the state of the vth vertex that has been updated conditioned on the other units\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array of values that the prior distribution is based from.\n",
    "    - b: a (1, v) numpy array of bias\n",
    "    - v: (int) the state of the vertex to be updated.\n",
    "    \"\"\"\n",
    "    stateSize = initialState.shape\n",
    "    newState = np.zeros(stateSize) + initialState\n",
    "    prob = sigmoid(initialState.dot(W) + b)\n",
    "    newState[0, v] = np.random.binomial(1, prob[0, v], 1)\n",
    "    return newState, newState[0, v]\n",
    "\n",
    "\n",
    "def rand_gibbs_sample(initialState, W, b, n):\n",
    "    \"\"\"\n",
    "    Does a random scan Gibbs sampling n times with a given initial state, weight matrix W and bias b.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array.\n",
    "    - b: a (1, v) numpy array of bias\n",
    "    - n: (int) number of samples to be generated.\n",
    "    \"\"\"\n",
    "    for i in range(n):\n",
    "        s = np.random.randint(0, v)\n",
    "        initialState, vertexState = single_unit_update(initialState, W, b, s)\n",
    "    return initialState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make ensure that the sample that we obtain are independent and identically distributed, we do a **burn-in** of $10000\\times v$ iterations so that the samples obtained follow the distribution of the weight matrix, following which we pick a sample for every $1000 \\times v$ iterations, which is called **mixing-in**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def burnin(initialState, W, b):\n",
    "    \"\"\"\n",
    "    Performs burn in of 10000 x v iterations.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array.\n",
    "    - b: a (1, v) numpy array of bias\n",
    "    \"\"\"\n",
    "    v = W.shape[0]\n",
    "    burnin_state = rand_gibbs_sample(initialState, W, b, 10000 * v)\n",
    "    print ('Burn-in state: ', burnin_state)\n",
    "    return burnin_state\n",
    "\n",
    "\n",
    "def mixin_gibbs_sample(initialState, W, b, n, m, savesamples = 'True'):\n",
    "    \"\"\"\n",
    "    Does a random scan Gibbs sampling n * m times with a given initial state and weight matrix W and \n",
    "    stores a sample every m iterations.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array. \n",
    "    - n: (int) number of samples to be drawn.\n",
    "    - m: (int) number of iterations before a sample is drawn.\n",
    "    - savedate: (bool) save samples as 'samples.dat.npy' if True and does not save if false.\n",
    "    \"\"\"\n",
    "    tic = time.time()\n",
    "           \n",
    "    v = W.shape[0]\n",
    "    sample = np.zeros((n, initialState.shape[1]))\n",
    "    for i in range(n):\n",
    "        initialState = rand_gibbs_sample(initialState, W, b, m)\n",
    "        sample[i] = initialState\n",
    "    if savesamples == \"True\":\n",
    "        np.save('gibbs-sample.dat', sample)\n",
    "        print ('Samples are saved as \"gibbs-sample.dat.npy\"')\n",
    "    elif savesamples == \"False\":\n",
    "        print ('Samples were not saved. Run np.save(\"gibbs-sample.dat\", sample) to save them. ')\n",
    "    else:\n",
    "        raise ValueError(\"savesamples must be 'True' or 'False'\")\n",
    "    \n",
    "    toc = time.time()\n",
    "    print ('Time taken to create %d samples is %f minutes' % (n, (toc - tic)/60))\n",
    "    return sample\n",
    "\n",
    "def makesamples(initialState, W, b, n, m, savesamples = 'True'):\n",
    "    \"\"\"\n",
    "    Make samples.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array. \n",
    "    - n: (int) number of samples to be drawn.\n",
    "    - m: (int) number of iterations before a sample is drawn.\n",
    "    - savedate: (bool) save samples as 'samples.dat.npy' if True and does not save if false.\n",
    "    \"\"\"\n",
    "    b = burnin(initialState, W, b)\n",
    "    samples = mixin_gibbs_sample(b, W, b, n, m, savesamples)\n",
    "    return samples  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "initialState = initializestates(v, p)\n",
    "W = initializeW(v)\n",
    "b = initializeb(v)\n",
    "\n",
    "\n",
    "t = makesamples(initialState, W, b,  50000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
