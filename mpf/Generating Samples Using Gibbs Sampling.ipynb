{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Samples\n",
    "----\n",
    "### Introduction\n",
    "Here we want to generate binary samples of length $v$ using Gibbs sampling based on a prior distribution based on a $v \\times v$ matrix $W$ initialized from a known distribution. Using the samples generated, we would eventually want to reproduce/learn the matrix $W$ from the samples using minimum probability flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import time\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall consider a network with $v$ vertices where each vertex is binary. For a network with $v$ vertices, there are $2^v$ possibles binary states. We initialize an initial state of the network using the Bernoulli distribution with success probability $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of neurons:\n",
    "v = 16\n",
    "# Success probability:\n",
    "p = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial states:  [1 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "initialState = np.random.binomial(1, p, v)\n",
    "print ('Initial states: ',initialState)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now initialize a $v$ by $v$ matrix $W$ with each entry drawn from a standard normal distribution, $N(0,1)$. For each entry in the matrix, $W_{ij}$ denotes the parameter/weight associated with the connection from unit $i$ to $j$ (can we think of it as the conditional weight of $v^{(t+1)}_i=1$ given $v^{(t)}_j=1$?). Here we save the matrix $W$ so we can verify the learning by MPF. \n",
    "\n",
    "(Personal notes: Later, we will learn that initializing the matrix $W$ with zero diagonals will make it easier in the generation of samples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.94711814  0.04547612  0.31349768  1.02249207 -1.07121386\n",
      "   0.48029422 -0.50629145  0.28453795  1.39687164 -0.17699444  0.32543192\n",
      "   0.36087745  0.20079981 -0.54846689 -0.15188135]\n",
      " [ 0.94711814 -0.20515826 -0.83386438 -0.53341801 -1.47738538  0.77722254\n",
      "   1.32515335  0.58394997  1.33860823 -0.25494309  0.98764529 -0.43836681\n",
      "   0.18234286  0.68560419  0.10910471 -0.00955465]\n",
      " [ 0.04547612 -0.83386438 -0.34791215 -0.3695588  -0.19995383  0.83402114\n",
      "  -0.13020736 -0.28515306 -0.90923452 -1.16642008 -0.37842469  0.74762145\n",
      "   0.00681224  0.2360523  -0.54814441  0.32230489]\n",
      " [ 0.31349768 -0.53341801 -0.3695588   0.3869025  -0.02401144 -1.35843794\n",
      "   0.18690357  0.61539413  0.30288321  0.70974409 -0.27338543 -0.172631\n",
      "  -0.42200339 -0.02116747 -0.29048262 -1.01203674]\n",
      " [ 1.02249207 -1.47738538 -0.19995383 -0.02401144 -0.90729836  0.77009879\n",
      "   1.30612063  0.53811744  0.23253401 -1.27536662 -0.41154694 -0.519402\n",
      "  -0.0495033   0.10373915 -0.4544343  -0.12642381]\n",
      " [-1.07121386  0.77722254  0.83402114 -1.35843794  0.77009879  1.89588918\n",
      "  -0.08448974 -0.16746746 -0.51946103  0.29643356  0.22059301 -0.07625311\n",
      "   0.51588957  0.25510997  0.05922233 -0.48471532]\n",
      " [ 0.48029422  1.32515335 -0.13020736  0.18690357  1.30612063 -0.08448974\n",
      "  -1.270485    0.79173804 -0.90448474  0.93768979 -0.70691716 -0.69553661\n",
      "   2.04308899  0.26803427  0.22174903  1.02918811]\n",
      " [-0.50629145  0.58394997 -0.28515306  0.61539413  0.53811744 -0.16746746\n",
      "   0.79173804  0.92220667  0.52642941  0.30697097 -0.62326646 -0.44850863\n",
      "   0.32098004 -0.27984212 -0.46423672  1.46444116]\n",
      " [ 0.28453795  1.33860823 -0.90923452  0.30288321  0.23253401 -0.51946103\n",
      "  -0.90448474  0.52642941  0.57659082 -0.47652679  0.79201825 -0.23391503\n",
      "  -0.93021972  0.21118564 -0.18809399 -0.08916641]\n",
      " [ 1.39687164 -0.25494309 -1.16642008  0.70974409 -1.27536662  0.29643356\n",
      "   0.93768979  0.30697097 -0.47652679 -0.82643854  0.10924504 -1.13276797\n",
      "   0.44362837 -0.35038989 -0.3657093  -0.95212219]\n",
      " [-0.17699444  0.98764529 -0.37842469 -0.27338543 -0.41154694  0.22059301\n",
      "  -0.70691716 -0.62326646  0.79201825  0.10924504  0.92085882 -0.39282784\n",
      "   0.9782451   0.80414168 -1.09521264  0.6013297 ]\n",
      " [ 0.32543192 -0.43836681  0.74762145 -0.172631   -0.519402   -0.07625311\n",
      "  -0.69553661 -0.44850863 -0.23391503 -1.13276797 -0.39282784  0.05216508\n",
      "  -0.04214963  0.75037872 -0.2558294  -0.15436855]\n",
      " [ 0.36087745  0.18234286  0.00681224 -0.42200339 -0.0495033   0.51588957\n",
      "   2.04308899  0.32098004 -0.93021972  0.44362837  0.9782451  -0.04214963\n",
      "   0.64013153 -1.28645052  0.73507921 -0.29803887]\n",
      " [ 0.20079981  0.68560419  0.2360523  -0.02116747  0.10373915  0.25510997\n",
      "   0.26803427 -0.27984212  0.21118564 -0.35038989  0.80414168  0.75037872\n",
      "  -1.28645052 -0.34598178 -1.2667905   0.08107653]\n",
      " [-0.54846689  0.10910471 -0.54814441 -0.29048262 -0.4544343   0.05922233\n",
      "   0.22174903 -0.46423672 -0.18809399 -0.3657093  -1.09521264 -0.2558294\n",
      "   0.73507921 -1.2667905   0.42625873  0.68422339]\n",
      " [-0.15188135 -0.00955465  0.32230489 -1.01203674 -0.12642381 -0.48471532\n",
      "   1.02918811  1.46444116 -0.08916641 -0.95212219  0.6013297  -0.15436855\n",
      "  -0.29803887  0.08107653  0.68422339  0.69474914]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# Get a symmetric matrix with diagonal all zeros\n",
    "W = np.random.normal(0, 1, (v,v))\n",
    "W = 0.5 * (W + np.transpose(W))\n",
    "\n",
    "# To save and load W matrix\n",
    "np.save('W.dat', W)\n",
    "# W = np.load('W.dat')\n",
    "print (W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do Gibbs Sampling\n",
    "The reason for doing Gibbs sampling is to generate samples $\\mathcal{S}$ from known parameters $W$ and then use MPF to learn the parameters $W$ using $\\mathcal{S}$. To sample from this multivariate distribution, we start with an initial state obtained from a prior belief, following which sampling from the conditional distribution is done to get a new state of a **vertex**. Thus if we were to sample each vertex sequentially, a network with $v$ vertices would require sampling from (different) conditional distributions $v$ times for a new state of the network to be obtained.\n",
    "\n",
    "#### Algorithm: Gibbs sampler (cycle)\n",
    "1. Initialize $\\mathbf{x^{(0)}}=(x_1^{(0)},\\ldots,x_v^{(0)})$ base on some prior belief.\n",
    "2. For $i = 1,2, \\ldots$\n",
    "    - sample $X_1^{(i)}\\sim \\mathbb{P}(X_1^{(i)}=x_1^{(i)}\\mid X_2=x_2^{(i-1)},\\ldots,X_v=x_v^{(i-1)})$\n",
    "    - sample $X_2^{(i)}\\sim \\mathbb{P}(X_2^{(i)}=x_2^{(i)}\\mid X_1=x_1^{(i)},X_3=x_3^{(i-1)}\\ldots,X_v=x_v^{(i-1)})$\n",
    "    - in general, sample $X_j^{(i)}\\sim \\mathbb{P}(X_{j}^{(i)}=x_{j}^{(i)}\\mid X_1=x_1^{(i)},\\ldots, X_{j-1}=x_{j-1}^{(i)},X_{j+1}=x_{j+1}^{(i-1)},\\ldots,X_v=x_v^{(i-1)})$ for $j=1,2, \\ldots v$, which then generates a new state for the network.\n",
    "\n",
    "There is also another variation of the Gibbs sampler called the **random scan** where the update of the state of the vertex is not in a cycle but done in a random manner. Below are some functions that are defined for the implementation of the Gibbs sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Takes in a vector x and returns its sigmoid activation.\n",
    "    Input:\n",
    "    - x: a numpy array\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def single_unit_update(initialState, W, v):\n",
    "    \"\"\"\n",
    "    Returns the new states and the state of the vth vertex that has been updated conditioned on the other units\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array of values that the prior distribution is based from. \n",
    "    - v: (int) the state of the vertex to be updated.\n",
    "    \"\"\"\n",
    "    stateSize = initialState.shape[0]\n",
    "    newState = initialState\n",
    "#     Here we see that to update a single vertex state we only use the weights Wij for i not \n",
    "#     equal to j and hence the reason to set the diagonals to be zero earlier. But since \n",
    "#     we did not we have to kill off the diagonals of W here.\n",
    "    prob = sigmoid((W - (W * np.eye(stateSize))).dot(initialState))\n",
    "    newState[v] = np.random.binomial(1, prob[v], 1)\n",
    "#     print (initialState[n], newState[n])\n",
    "    return newState, newState[v]\n",
    "\n",
    "\n",
    "def gibbs_sample(initialState, W):\n",
    "    \"\"\"\n",
    "    Returns the new state of the network after updating all v units systematically, given an initialized state \n",
    "    of the network and weight matrix W.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array.\n",
    "    \"\"\"\n",
    "#     print ('initialState:', initialState)\n",
    "    stateSize = initialState.shape\n",
    "    newState = np.zeros(stateSize)\n",
    "    for i in range(stateSize[0]):\n",
    "#         print ('Changing the state for unit %d...'% i)\n",
    "        initialState, vertexState = single_unit_update(initialState, W, i)\n",
    "#         print ('Old unit state is %d, new unit state is %d'% (initialState[i], unitState))\n",
    "        newState[i] = vertexState  \n",
    "#     print ('newState:', newState)\n",
    "    return newState    \n",
    "\n",
    "\n",
    "def multi_gibbs_sample(initialState, W, n):\n",
    "    \"\"\"\n",
    "    Performs Gibbs sampling n times with a given initial state and weight matrix W\n",
    "    and stores each sample as a row.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array. \n",
    "    - n: (int) number of samples to be drawn.\n",
    "    \"\"\"\n",
    "    stateSize = initialState.shape[0]\n",
    "    sample = np.zeros((n, stateSize))\n",
    "    for i in range(n):\n",
    "        sample[i, :] = gibbs_sample(initialState, W)\n",
    "    return sample    \n",
    "\n",
    "def rand_gibbs_sample(initialState, W, n):\n",
    "    \"\"\"\n",
    "    Does a random scan Gibbs sampling n times with a given initial state and weight matrix W.\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array. \n",
    "    - n: (int) number of samples to be drawn.\n",
    "    \"\"\"\n",
    "#     v = W.shape[0]\n",
    "#     sample = np.zeros((n, initialState.shape[0]))\n",
    "    for i in range(n):\n",
    "        s = np.random.randint(0, v)\n",
    "        initialState, vertexState = single_unit_update(initialState, W, s)\n",
    "#         sample[i, :] = gibbs_sample(initialState, W)\n",
    "    return initialState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To make ensure that the sample that we obtain are independent and identically distributed, we do a **burn-in** of $10000\\times v$ iterations so that the samples obtained follow the distribution of the weight matrix, following which we pick a sample for every $1000 \\times v$ iterations, which is called **mixing-in**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling using random scan Gibbs sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burn-in state:  [1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Burn-in\n",
    "burnin_state = rand_gibbs_sample(initialState, W, 10000 * v)\n",
    "print ('Burn-in state: ', burnin_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixing in still halfway done..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mixing-in\n",
    "def mixin_gibbs_sample(initialState, W, n, m):\n",
    "    \"\"\"\n",
    "    Does a random scan Gibbs sampling n * m times with a given initial state and weight matrix W and \n",
    "    stores a sample every m iterations.\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array. \n",
    "    - n: (int) number of samples to be drawn.\n",
    "    \"\"\"\n",
    "    tic = time.time()\n",
    "    v = W.shape[0]\n",
    "    sample = np.zeros((n, initialState.shape[0]))\n",
    "    for i in range(n):\n",
    "#         if i % 5000 == 0:\n",
    "#             print ('Creating %d sample' % (i))\n",
    "        sample[i, :] = rand_gibbs_sample(initialState, W, m)\n",
    "#         sample[i, :] = \n",
    "#         if i % m == 0:\n",
    "            \n",
    "#         s = np.random.randint(0, v)\n",
    "#         initialState, vertexState = single_unit_update(initialState, W, s)\n",
    "#         if i % m == 0:\n",
    "#             sample[i, :] = gibbs_sample(initialState, W)\n",
    "    toc = time.time()\n",
    "    print ('Time taken to create %d samples is %f minutes' % (n, (toc - tic)/60))\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to create 50000 samples is -3.001808\n"
     ]
    }
   ],
   "source": [
    "sample = mixin_gibbs_sample(burnin_state, W, 50000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
