{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import time\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall consider a network with $v$ vertices where each vertex is binary. For a network with $v$ vertices, there are $2^v$ possibles binary states. We initialize an initial state of the network using the Bernoulli distribution with success probability $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of neurons:\n",
    "v = 16\n",
    "# Success probability:\n",
    "p = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Needs to be edited later!!!\n",
    "We now initialize a $v$ by $v$ matrix $W$ with each entry drawn from a standard normal distribution, $N(0,1)$. For each entry in the matrix, $W_{ij}$ denotes the parameter/weight associated with the connection from unit $i$ to $j$ (can we think of it as the conditional weight of $v^{(t+1)}_i=1$ given $v^{(t)}_j=1$?). Here we save the matrix $W$ so we can verify the learning by MPF. \n",
    "\n",
    "(Personal notes: Later, we will learn that initializing the matrix $W$ with zero diagonals will make it easier in the generation of samples.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initializestates(v, p):\n",
    "    \"\"\"\n",
    "    Initializes a v by 1 array of initial states\n",
    "    Input:\n",
    "    - v: (int) number of neurons in the system.\n",
    "    - p: (probability) probability of getting a 1.\n",
    "    \"\"\"\n",
    "    initialState = np.random.binomial(1, p, v)\n",
    "    initialState = initialState.reshape(1, v)\n",
    "    return initialState\n",
    "\n",
    "\n",
    "def initializeW(v):\n",
    "    \"\"\"\n",
    "    Initializes a v by v matrix of values. For now we can think of it\n",
    "    in the scenario of a Ising model where W describes the interaction\n",
    "    between the different nodes.\n",
    "    Input:\n",
    "    - v: (int) number of neurons in the system.\n",
    "    \"\"\"\n",
    "    W = np.random.normal(0, 1, (v, v))\n",
    "    W = np.triu(W, 1)+np.triu(W, 1).T\n",
    "    # To save and load W matrix\n",
    "    np.save('W.dat', W)\n",
    "    # W = np.load('W.dat')\n",
    "    print ('Initialized W matrix: \\n', W)\n",
    "    return W\n",
    "\n",
    "\n",
    "def initializeb(v):\n",
    "    \"\"\"\n",
    "    Initializes a 1 by v array of values. For now we can think of it\n",
    "    some form of bias in the system.\n",
    "    Input:\n",
    "    - v: (int) number of neurons in the system.\n",
    "    \"\"\"\n",
    "    b = np.zeros((1,v))\n",
    "    # W = np.random.normal(0, 1, (v,v))\n",
    "    # W = 0.5 * (W + np.transpose(W))\n",
    "    # W = W - np.diag(W)\n",
    "\n",
    "    # To save and load W matrix\n",
    "    # np.save('W.dat', W)\n",
    "    # W = np.load('W.dat')\n",
    "    print ('Initialized bias: ', b)\n",
    "    return b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to do Gibbs Sampling\n",
    "T\n",
    "histributions $v$ times for a new state of the network to be obtained., preason for doing Gibbs sampling is to generate samples $\\mathcal{S}$ from known parameters $W$ and then use MPF to learn the parameters $W$ using $\\mathcal{S}$. To sample from this multivariate distribution, we start with an initial state obtained from a prior belief following which sampling from the conditional distribution is done to get a new state of a **vertex**. \n",
    "\n",
    "#### Algorithm: Gibbs sampler (random scan)\n",
    "1. Initialize $\\mathbf{x^{(0)}}=(x_1^{(0)},\\ldots,x_v^{(0)})$ base on some prior belief.\n",
    "2. For $i = 1,2, \\ldots$, pick a random integer $k$ from $1 , \\ldots, v$ then \n",
    "    - sample $X_k^{(i)}\\sim \\mathbb{P}(X_k^{(i)}=x_1^{(i)}\\mid X_1=x_1^{(i-1)},X_2=x_2^{(i-1)},\\ldots,X_{k-1}=x_{k-1}^{(i-1)},X_{k+1}=x_{k+1}^{(i-1)},\\ldots,X_v=x_v^{(i-1)})$\n",
    "which gives you a new state of the network.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Takes in a vector x and returns its sigmoid activation.\n",
    "    Input:\n",
    "    - x: a numpy array\n",
    "    \"\"\"\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def single_unit_update(initialState, W, b, v):\n",
    "    \"\"\"\n",
    "    Returns the new states and the state of the vth vertex that has been updated conditioned on the other units\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array of values that the prior distribution is based from.\n",
    "    - b: a (1, v) numpy array of bias\n",
    "    - v: (int) the state of the vertex to be updated.\n",
    "    \"\"\"\n",
    "    stateSize = initialState.shape\n",
    "    newState = np.zeros(stateSize) + initialState\n",
    "    prob = sigmoid(initialState.dot(W) + b)\n",
    "    newState[0, v] = np.random.binomial(1, prob[0, v], 1)\n",
    "    return newState, newState[0, v]\n",
    "\n",
    "\n",
    "def rand_gibbs_sample(initialState, W, b, n):\n",
    "    \"\"\"\n",
    "    Does a random scan Gibbs sampling n times with a given initial state, weight matrix W and bias b.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array.\n",
    "    - b: a (1, v) numpy array of bias\n",
    "    - n: (int) number of samples to be generated.\n",
    "    \"\"\"\n",
    "    for i in range(n):\n",
    "        s = np.random.randint(0, v)\n",
    "        initialState, vertexState = single_unit_update(initialState, W, b, s)\n",
    "    return initialState"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make ensure that the sample that we obtain are independent and identically distributed, we do a **burn-in** of $10000\\times v$ iterations so that the samples obtained follow the distribution of the weight matrix, following which we pick a sample for every $1000 \\times v$ iterations, which is called **mixing-in**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def burnin(initialState, W, b):\n",
    "    \"\"\"\n",
    "    Performs burn in of 10000 x v iterations.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array.\n",
    "    - b: a (1, v) numpy array of bias\n",
    "    \"\"\"\n",
    "    v = W.shape[0]\n",
    "    burnin_state = rand_gibbs_sample(initialState, W, b, 10000 * v)\n",
    "    print ('Burn-in state: ', burnin_state)\n",
    "    return burnin_state\n",
    "\n",
    "\n",
    "def mixin_gibbs_sample(initialState, W, b, n, m, savesamples = 'True'):\n",
    "    \"\"\"\n",
    "    Does a random scan Gibbs sampling n * m times with a given initial state and weight matrix W and \n",
    "    stores a sample every m iterations.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array. \n",
    "    - n: (int) number of samples to be drawn.\n",
    "    - m: (int) number of iterations before a sample is drawn.\n",
    "    - savedate: (bool) save samples as 'samples.dat.npy' if True and does not save if false.\n",
    "    \"\"\"\n",
    "    tic = time.time()\n",
    "           \n",
    "    v = W.shape[0]\n",
    "    sample = np.zeros((n, initialState.shape[1]))\n",
    "    for i in range(n):\n",
    "        initialState = rand_gibbs_sample(initialState, W, b, m)\n",
    "        sample[i] = initialState\n",
    "    if savesamples == \"True\":\n",
    "        np.save('gibbs-sample.dat', sample)\n",
    "        print ('Samples are saved as \"gibbs-sample.dat.npy\"')\n",
    "    elif savesamples == \"False\":\n",
    "        print ('Samples were not saved. Run np.save(\"gibbs-sample.dat\", sample) to save them. ')\n",
    "    else:\n",
    "        raise ValueError(\"savesamples must be 'True' or 'False'\")\n",
    "    \n",
    "    toc = time.time()\n",
    "    print ('Time taken to create %d samples is %f minutes' % (n, (toc - tic)/60))\n",
    "    return sample\n",
    "\n",
    "def makesamples(initialState, W, b, n, m, savesamples = 'True'):\n",
    "    \"\"\"\n",
    "    Make samples.\n",
    "    Input:\n",
    "    - initialState: a numpy array of binary values denoting the initial state of the nodes.\n",
    "    - W: a 2d numpy array. \n",
    "    - n: (int) number of samples to be drawn.\n",
    "    - m: (int) number of iterations before a sample is drawn.\n",
    "    - savedate: (bool) save samples as 'samples.dat.npy' if True and does not save if false.\n",
    "    \"\"\"\n",
    "    b = burnin(initialState, W, b)\n",
    "    samples = mixin_gibbs_sample(b, W, b, n, m, savesamples)\n",
    "    return samples  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "initialState = initializestates(v, p)\n",
    "W = initializeW(v)\n",
    "b = initializeb(v)\n",
    "\n",
    "\n",
    "t = makesamples(initialState, W, b,  50000, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized W matrix: \n",
      " [[ 0.          0.23315981 -0.41442199  0.64734756 -1.09865496  0.18951477\n",
      "   0.67176322 -0.50910919 -0.20054966 -0.64993808 -0.08537763  0.43167974\n",
      "   0.55934658  0.0693222   0.68748319  1.57273294]\n",
      " [ 0.23315981  0.         -1.64633056  2.11321877  1.95552244  1.19651397\n",
      "   0.65851845  0.21703923  0.56161409  0.91434768  0.81798816 -0.374813\n",
      "  -0.94486193 -1.90613505  0.55496683  0.64158013]\n",
      " [-0.41442199 -1.64633056  0.          1.18725173  0.73725632  1.67308946\n",
      "  -0.58135902 -0.47021966  0.35275579  0.52580778 -1.01085213  0.11835908\n",
      "   0.96472162  0.79835586 -0.62715385  2.17213668]\n",
      " [ 0.64734756  2.11321877  1.18725173  0.         -0.21717797 -1.04710238\n",
      "  -0.06816384  1.41845501  0.86562215  1.3496813   1.00354722  0.17350864\n",
      "  -0.30145845 -0.0944941   1.07682338  0.15071019]\n",
      " [-1.09865496  1.95552244  0.73725632 -0.21717797  0.          1.40861692\n",
      "   0.55773197 -1.08440271 -0.2503212   0.06446465 -1.39949638  0.57437861\n",
      "   1.25818384 -0.74160247 -0.17637902 -0.53453833]\n",
      " [ 0.18951477  1.19651397  1.67308946 -1.04710238  1.40861692  0.\n",
      "   0.73595595  1.25988716 -0.90065437  0.09945627  1.24810217 -2.02198831\n",
      "   0.75163291  1.2792074  -1.81995037  1.35097558]\n",
      " [ 0.67176322  0.65851845 -0.58135902 -0.06816384  0.55773197  0.73595595\n",
      "   0.          1.17556396  0.20692294 -0.25584246  1.24562962 -0.18580715\n",
      "   0.64272213 -0.13720694 -2.11419674  0.70496438]\n",
      " [-0.50910919  0.21703923 -0.47021966  1.41845501 -1.08440271  1.25988716\n",
      "   1.17556396  0.          0.24611118  0.55577846 -0.08163541  0.98412872\n",
      "   1.45680732 -0.87662789 -0.42247778 -1.34267548]\n",
      " [-0.20054966  0.56161409  0.35275579  0.86562215 -0.2503212  -0.90065437\n",
      "   0.20692294  0.24611118  0.         -1.24854093  0.04053905 -0.06129575\n",
      "  -1.87474649  0.55198307 -0.89824095 -0.69185392]\n",
      " [-0.64993808  0.91434768  0.52580778  1.3496813   0.06446465  0.09945627\n",
      "  -0.25584246  0.55577846 -1.24854093  0.          0.08819114 -0.06164422\n",
      "   0.44923294  0.10801257 -2.50025283 -0.65002279]\n",
      " [-0.08537763  0.81798816 -1.01085213  1.00354722 -1.39949638  1.24810217\n",
      "   1.24562962 -0.08163541  0.04053905  0.08819114  0.         -1.20905699\n",
      "   0.68659398  0.64090167  0.55068045  0.48642476]\n",
      " [ 0.43167974 -0.374813    0.11835908  0.17350864  0.57437861 -2.02198831\n",
      "  -0.18580715  0.98412872 -0.06129575 -0.06164422 -1.20905699  0.\n",
      "   1.24633198  0.80770757  0.61418569  0.2199009 ]\n",
      " [ 0.55934658 -0.94486193  0.96472162 -0.30145845  1.25818384  0.75163291\n",
      "   0.64272213  1.45680732 -1.87474649  0.44923294  0.68659398  1.24633198\n",
      "   0.          0.67501319  0.25768415  0.83792007]\n",
      " [ 0.0693222  -1.90613505  0.79835586 -0.0944941  -0.74160247  1.2792074\n",
      "  -0.13720694 -0.87662789  0.55198307  0.10801257  0.64090167  0.80770757\n",
      "   0.67501319  0.          2.89047884  1.12938936]\n",
      " [ 0.68748319  0.55496683 -0.62715385  1.07682338 -0.17637902 -1.81995037\n",
      "  -2.11419674 -0.42247778 -0.89824095 -2.50025283  0.55068045  0.61418569\n",
      "   0.25768415  2.89047884  0.         -0.28022056]\n",
      " [ 1.57273294  0.64158013  2.17213668  0.15071019 -0.53453833  1.35097558\n",
      "   0.70496438 -1.34267548 -0.69185392 -0.65002279  0.48642476  0.2199009\n",
      "   0.83792007  1.12938936 -0.28022056  0.        ]]\n",
      "Initialized bias:  [[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "Burn-in state:  [[ 1.  0.  1.  1.  0.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0.  1.]]\n",
      "Samples are saved as \"gibbs-sample.dat.npy\"\n",
      "Time taken to create 50000 samples is 1.093333 minutes\n"
     ]
    }
   ],
   "source": [
    "run mpfgibbs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
